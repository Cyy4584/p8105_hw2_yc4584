---
title: "p8105_hw2_yc4584"
author: "Yingyu Cui"
date: "2024-09-30"
output: github_document
---

```{r set up}
library(tidyverse)
```


# Problem 1: NYC Transit Data

Read and clean the data
```{r read and clean the data}
subway_df = 
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = "") |> 
  janitor::clean_names() |> 
  select(line:entry, vending, ada) |>
  mutate(entry = case_match(entry, 
                            "YES" ~ TRUE,
                            "NO" ~ FALSE))
```
For this data set, it contains the variables "line", "station_name",  "routes", "station_latitude", "station_longitude", "entrance_type", "entry", "vending" and "ada" columns. The "line", "station_name", "route1-7", "entrance_type" and "vending" are character variables, the "station_latitude", "station_longitude", "route8-11" are numeric variables, and the "entry" and "ada" are logical variables.
For data cleaning, I used the janitor package to clean the column names and the select function to select the columns I need. I also used the case_match function to convert the "entry" column to logical variables.
The dimension of the cleaned data set is 1868 rows and 19 columns.
Except the columns of routes belonging to different kinds of variables, I think the data set is pretty much clean.

Question1: 
```{r distinct stations}
distinct_stations = subway_df |> 
  distinct(line, station_name)
num_distinct_stations = nrow(distinct_stations)
print(num_distinct_stations)
```
There are 465 distinct stations in the data set.

Question2:
```{r ADA compliance}
num_ada_compliance = subway_df |> 
  filter(ada == "TRUE") |> 
  nrow()
print(num_ada_compliance)
```
Therefore, 468 stations are ADA compliant.

Question3:
```{r vending}
num_without_vending = subway_df |> 
  filter(vending == "NO" & entry == "TRUE") |> 
  nrow()
print(num_without_vending)
```
Therefore, the proportion is 69/1867 = 0.037.

Reformat data so that route number and route name are distinct variables.
```{r reformat data}
subway_df_reformat = subway_df |> 
  mutate(across(starts_with("route"), as.character)) |> 
  pivot_longer(cols = starts_with("route"),
               names_to = "route_number", 
               values_to = "route_name",
               values_drop_na = TRUE)
```
How many serve the A train?
```{r A train}
num_A_train = subway_df_reformat |> 
  filter(route_name == "A") |> 
  distinct(line, station_name) |> 
  nrow()
print(num_A_train)
```
Therefore,60 distinct stations serve the A train.

Of the stations that serve the A train, how many are ADA compliant?
```{r A train ADA}
num_A_train_ADA = subway_df_reformat |> 
  filter(route_name == "A" & ada == "TRUE") |> 
  distinct(line, station_name) |> 
  nrow()
print(num_A_train_ADA)
```
Therefore, 17 stations that serve the A train are ADA compliant.


# Problem 2: Mr. Trash Wheel
```{r set up2}
library(readxl)
```
Read and clean the data of Mr. Trash Wheel
```{r read and clean the data of Mr. Trash Wheel}
trash_df = 
  read_excel("./data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", range = "A2:N653") |> 
  janitor::clean_names() |>
  mutate(sports_balls = as.integer(round(sports_balls))) |> 
  mutate(trash_wheel = "Mr. Trash Wheel") |> 
  mutate(year = as.numeric(year))
```
Read and clean the data of Professor Trash Wheel
```{r read and clean the data of Professor Trash Wheel}
trash_df_professor = 
  read_excel("./data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", range = "A2:M120") |> 
  janitor::clean_names() |> 
  mutate(trash_wheel = "Professor Trash Wheel") |> 
  mutate(year = as.numeric(year))
```
Read and clean the data of Gwynnda
```{r read and clean the data of Gwynnda}
trash_df_gwynnda = 
  read_excel("./data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", range = "A2:L265") |> 
  janitor::clean_names() |> 
  mutate(trash_wheel = "Gwynnda Trash Wheel") |> 
  mutate(year = as.numeric(year))
```
Combine the data frames 
```{r combine the data frames}
trash_df_tidy = bind_rows(trash_df, trash_df_professor, trash_df_gwynnda)
```
number of observations in the combined data frame
```{r number of observations}
total_observations <- nrow(trash_df_tidy)
```
Therefore, the total number of observations in the combined data frame is 1032.
examples of these variables
```{r examples of these variables}
str(trash_df_tidy)
```
The variables in the combined data frame are "dumpster", "year", "month", "data", "weight_tons", "volume_cubic_yards", "plastic_bottles", "polystyrene", "cigarette_butts", "glass_bottles", "plastic_bags", "wrappers", "sports_balls", "homes_powered" and "trash_wheel". The "month" and "trash_wheel" are character variables and the rest are numeric variables except date.

Question1:what was the total weight of trash collected by Professor Trash Wheel? 
```{r total weight of trash collected by Professor Trash Wheel}
total_weight_professor = trash_df_tidy |> 
  filter(trash_wheel == "Professor Trash Wheel") |> 
  summarize(total_weight_professor = sum(weight_tons)) |> 
  pull(total_weight_professor)
print(total_weight_professor)
```
Therefore, the total weight of trash collected by Professor Trash Wheel is 246.74 tons.

Question2:  What was the total number of cigarette butts collected by Gwynnda in June of 2022?
```{r total number of cigarette butts collected by Gwynnda in June of 2022}
total_cigarettes_gwynnda = trash_df_tidy |> 
  filter(trash_wheel == "Gwynnda Trash Wheel" & month == "June" & year == 2022) |> 
  summarize(total_cigarettes_gwynnda = sum(cigarette_butts)) |> 
  pull(total_cigarettes_gwynnda)
print(total_cigarettes_gwynnda)
```
Therefore, the total number of cigarette butts collected by Gwynnda in June of 2022 is 18120.


# Problem 3: Great British Bake Off
import and clean the data sets for bakes
```{r import the data sets for bakes}
bakes_df = 
  read_csv("./data/bakes.csv", na = c("", "N/A")) |> 
  janitor::clean_names()
```
import and clean the data sets for bakers
```{r import the data sets for bakers}
bakers_df = 
  read_csv("./data/bakers.csv") |> 
  janitor::clean_names() |> 
  rename(baker = baker_name)
  bakers_df$baker = str_extract(bakers_df$baker, "^[^ ]+")
```
import and clean the data sets for results
```{r import the data sets for results}
results_df = 
  read_csv("./data/results.csv", skip = 2, na = c("", "NA")) |> 
  janitor::clean_names()
```
check the missing values in the bakers_df
```{r check the missing values1}
missing_result1 = anti_join( bakers_df, results_df, by = "baker", "series")
print(missing_result1)
```

check the missing values in the bakes_df
```{r check the missing values2}
missing_result2 = anti_join(bakes_df, results_df, by = "baker", "series")
print(missing_result2)
```

We found that the baker "Jo" in sery 2 is lost in the results_df. Therefore, we need to add the baker "Jo" to the results_df or omit it when combining the data frames.

combine and reorder the data frames
```{r combine and reorder the data frames}
full_df = 
  left_join(results_df, bakes_df, by = c("baker", "series", "episode")) |>   left_join(bakers_df) |> 
  select("baker", "baker_age", "baker_occupation", "hometown", "series", "episode", "signature_bake", everything())
```
export the new cvs.
```{r export the new cvs}
write_csv(full_df, "./data/full_df.csv")
```

Question1:
Describe your data cleaning process, including any questions you have or choices you made. Briefly discuss the final dataset.

For the data cleaning process, I used the read_csv function to read the data sets and set the NA and the janitor package to clean the column names. Besides, I rename the original baker_name column in the bakers_df into baker, which is for later combining. I also used the str_extract function to extract the first name of the bakers in the bakers_df. In the results_df, I skip the first 2 rows because it does not contain data but only some notes. After that, I used the anti_join function to check the missing values in the results_df. I found that the baker "Jo" in series 2 is lost. It is a question whether I need to add the baker "Jo" to the results_df or omit it when combining the data frames. Finally, I decided to use the left_join function to combine the data frames and the select function to reorder the columns so that the missing "Jo" could be omitted. For exporting csv, I used the write_csv function to export the new cvs.

The final data contains 1136 rows and 10 columns and is organized by baker names and other personal information. The variables in the final data set are "baker", "baker_age", "baker_occupation", "hometown", "series", "episode", "signature_bake", "technical", "result" and "showstopper". In them, the "baker", "baker_occupation", "hometown",  "signature_bake", "result" and "showstopper" are character variables, the "baker_age", "series", "episode" and "technical" are numeric variables. The baker column is pretty important because I use it as the reference column to combine the three frames. The data set is pretty much clean except for the missing "Jo" in the results_df. In cleaning, I choose to omit it when combining. This data frame could be used to fully understand the performance and personal information of the bakers in the Great British Bake Off.

Question2:
Create a reader-friendly table showing the star baker or winner of each episode in Seasons 5 through 10.
```{r create table}
star_baker_winner = full_df |> 
  filter(series >= 5 & series <= 10) |> 
  filter(result %in% c("STAR BAKER", "WINNER")) |> 
  group_by(series, episode) |> 
  select(baker, series, episode, result)
knitr::kable(star_baker_winner)
```
Comment on this table â€“ were there any predictable overall winners? Any surprises?
There are some predictable winners, like Nadiya in Season 6, Candice in Season 7. But there are also some surprises, like Nancy in Season 5, David in Season 10. Others in other seasons all had some powerful competitors, so it is hard to predict the overall winners.

Question3:
Import, clean, tidy, and organize the viewership data in viewers.csv. Show the first 10 rows of this dataset. 
```{r import the viewership data}
viewers_df = 
  read_csv("./data/viewers.csv", na = c("", "NA")) |>
  janitor::clean_names() |> 
  slice(1:10)
```
What was the average viewership in Season 1? In Season 5?
```{r average viewership 1}
average_value_1 = 
  mean(viewers_df$series_1, na.rm = TRUE)
print(average_value_1)
```
```{r average viewership 5}
average_value_5 = 
  mean(viewers_df$series_5, na.rm = TRUE)
print(average_value_5)
```
Therefore, the average viewership in Season 1 is 2.77 and the average viewership in Season 5 is 10.0393.
